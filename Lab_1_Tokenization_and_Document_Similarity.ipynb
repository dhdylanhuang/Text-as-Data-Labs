{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhdylanhuang/Text-as-Data-Labs/blob/main/Lab_1_Tokenization_and_Document_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbsRYu4r47Ee"
      },
      "source": [
        "# Text as Data Lab 1: Tokenization and Documentation Similarity\n",
        "\n",
        "The goal of the first lab is to find Reddit posts that are similar to our post of interest and learn about the standard text processing pipeline along the way.\n",
        "\n",
        "**Before you start, save a copy of this lab to your drive using \"File > Save a Copy in Drive\".** If you skip this step, you may lose the progress that you have made (e.g., if you close the browser tab or your computer crashes).\n",
        "\n",
        "This lab is linked with the first lecture. Please refer to the slides as needed.\n",
        "\n",
        "In this lab, you will learn about:\n",
        "- Tokenizing, stemming and other steps of processing text\n",
        "- Using spaCy for many of these steps\n",
        "- Calculating document similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yar9wJNoevRT"
      },
      "source": [
        "### Lab Code Testing\n",
        "\n",
        "Throughout the lab, you will be asked to complete the code for functions (highlighted by **Exercise**). You can test your code using the `labtest` function, which is specific to the TextAsData labs. This provides a small number of testcases to check if your code seems to be doing the right thing. They are not exhaustive tests.\n",
        "\n",
        "Before you can use it, you need to install it by running the next cell. This installs it from the [GitHub repo](https://github.com/jakelever/glasgowcs_labtest/) and loads the tests for this lab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FNU1G7Qt_v7"
      },
      "outputs": [],
      "source": [
        "# Installs the labtest system and loads the tests for this specific lab\n",
        "\n",
        "!pip install -U git+https://github.com/jakelever/glasgowcs_labtest.git\n",
        "from glasgowcs_labtest.textasdata.lab1 import labtest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLtRA8RIMp3o"
      },
      "source": [
        "\n",
        "Let's do a quick example of using the `labtest` function.\n",
        "\n",
        "**Exercise:** Complete the `multiply_by_three` function below so that it returns `3*x`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fl42PkfQZjUj"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZF5tWgnNFsh"
      },
      "source": [
        "Now run `labtest` with the function you want to test. `labtest` looks up the tests for that function and runs them. You'll get a (hopefully) useful error if your function doesn't match the expected test cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5KSJLrd1Xtm"
      },
      "outputs": [],
      "source": [
        "labtest(multiply_by_three)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In6bcBBTNXHN"
      },
      "source": [
        "Try rewriting your multiply_by_three to do the wrong thing (like multiply by two) and rerun `labtest` above.\n",
        "\n",
        "You can get more information about labtest, including the functions that it can test, by executing `?labtest`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UTN4w9k4EoM"
      },
      "outputs": [],
      "source": [
        "?labtest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiBAB2DONyB0"
      },
      "source": [
        "We'll use the `labtest` function to check a few of the functions that you write in this lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz-KUSAtjEEi"
      },
      "source": [
        "### Reddit Data\n",
        "\n",
        "In many of the labs, we will use data of posts from Reddit. This data was collected through the [Reddit API](https://www.reddit.com/dev/api/). Let's download it again (as we did in Lab 0) using wget. Recall that `!` at the beginning of a command tells Colab to run the command on the Linux system --- not in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w47ukM59_xsY"
      },
      "outputs": [],
      "source": [
        "!wget -O reddit_posts.json https://gla-my.sharepoint.com/:u:/g/personal/jake_lever_glasgow_ac_uk/EY_R8Y7DkrxMqXGe-zlgeNkBdJU5ZNTf8FYrN2pqDwddMA?download=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3XvFZpikHqJ"
      },
      "source": [
        "To load the file into Python, we need to use the JSON module as below. This loads it in a list of posts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8PYczeN0KdY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('reddit_posts.json') as f:\n",
        "  posts = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmGaBWu-kXHy"
      },
      "source": [
        "Always good to check what you've loaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8vUhf4xkVKh"
      },
      "outputs": [],
      "source": [
        "print(type(posts))\n",
        "print(len(posts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf9-f2uPlMD6"
      },
      "source": [
        "And what does a single post look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAdNyZyZlLYR"
      },
      "outputs": [],
      "source": [
        "posts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoCg44Hq7gU_"
      },
      "source": [
        "## The Similarity Problem\n",
        "\n",
        "We want to be able to measure the similarity between Reddit posts so that we can find the most similar posts for a particular post of interest.\n",
        "\n",
        "Here are the posts from the lecture: the post of interest and two options. Which one (A or B) do you think is closer to the post of interest?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbvAkTpu70xb"
      },
      "outputs": [],
      "source": [
        "target_post = posts[0]\n",
        "option_a = posts[1]\n",
        "option_b = posts[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqI5QB1U78Jm"
      },
      "outputs": [],
      "source": [
        "print('TARGET:',target_post['title'])\n",
        "print('-'*30)\n",
        "print('A:', option_a['title'])\n",
        "print('-'*30)\n",
        "print('B:', option_b['title'])\n",
        "print('-'*30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlASr9dddD0_"
      },
      "source": [
        "## Building our own Text Pipeline\n",
        "\n",
        "We need to tokenize and further process the text in these Reddit posts. We're going to go through each step to see what's involved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGcaSb4U8Don"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "In class, we decided that the number of overlapping tokens (which are similar to words) would be a useful measure of similarity. But first, we need the words.\n",
        "\n",
        "**Exercise:** Write a function (`tokenize_strsplit`) that splits text using whitespace. The [str.split](https://docs.python.org/3/library/stdtypes.html#str.split) function may be of help. The output for `tokenize_strsplit(\"I like irn bru\")` should be `[\"I\", \"like\", \"irn\", \"bru\"]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnGnmm1l7-Z0"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8EjoQRlO2w9"
      },
      "source": [
        "Use the `labtest` function to check the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AKB7GegFEOA"
      },
      "outputs": [],
      "source": [
        "labtest(tokenize_strsplit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz8_qizkF-uy"
      },
      "source": [
        "Let's try running it on a different sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K98h3Ar2ETsW"
      },
      "outputs": [],
      "source": [
        "print(tokenize_strsplit(\"Toto, I've got a feeling we're not in Kansas anymore.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmWEXGLkPBoG"
      },
      "source": [
        "Not bad, but as we talked about in class, there are some issues with punctuation. Note how the comma is attached to \"Toto\" and the full stop to \"anymore\". That's not ideal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYUiunyXHJnO"
      },
      "source": [
        "Let's work on our own tokenizer so we can come up with custom rules. Here is a rule-based tokenizer that also splits using spaces. At the moment, it has a similar output as `tokenize_strsplit` above. Take a look to see how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSpyC4OMF9v2"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def tokenize_whitespace_rule(text):\n",
        "  prev_c = ' ' # Keep track of the previous character (and set as a space initially)\n",
        "  tokens = []\n",
        "\n",
        "  # Loop through each character in the text\n",
        "  for c in text:\n",
        "\n",
        "    # Ignore characters that are whitespace (e.g. spaces, tabs, etc)\n",
        "    if not c in string.whitespace:\n",
        "\n",
        "      # If this is the start of a token (because the previous character is a space)\n",
        "      if prev_c in string.whitespace:\n",
        "        is_new_token = True\n",
        "      else: # This is continuation of a token\n",
        "        is_new_token = False\n",
        "\n",
        "      if is_new_token:\n",
        "        tokens.append(c)\n",
        "      else: # If we're not adding a new token, add the character to the last token\n",
        "        tokens[-1] += c\n",
        "\n",
        "    prev_c = c # Track what the previous character is\n",
        "\n",
        "  return tokens\n",
        "\n",
        "# Example usage:\n",
        "tokenize_whitespace_rule(\"The quick brown fox jumped over the lazy dog.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFPNYKvaDgeb"
      },
      "source": [
        "We're going to extend the idea from `tokenize_whitespace_rule` and add some extra rules to deal with things like punctuation.\n",
        "\n",
        "Specifically, a new token should start when:\n",
        "- Whitespace creates a new token\n",
        "  - e.g. 'a word' -> ['a', 'word']\n",
        "- There is a transition between letters (A-Z, a-z), numbers (0-9) or punctuation\n",
        "  - e.g. 'abc123.#.abc' => ['abc', '123', '.#.', 'abc']\n",
        "\n",
        "**Exercise:** Copy the code for `tokenize_whitespace_rule` above and modify it to create a function `tokenize_rulebased` below that splits on both whitespace and a change between letters, digits and punctuation.\n",
        "\n",
        "The output of `tokenize_rulebased` for the input `\"John's father didn't have $100.\"` should be `['John', \"'\", 's', 'father', 'didn', \"'\", 't', 'have', '$', '100', '.']`.\n",
        "\n",
        "Using some of the constants in the [string](https://docs.python.org/3/library/string.html) module, you can check if a character is punctuation, digits or text. For example, `\"m\" in string.ascii_letters` is `true`, `\"^\" in string.punctuation` is `true` and `\"7\" in string.digits` is also true."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbzeVQl0DR9F"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGIWK72MH0jY"
      },
      "outputs": [],
      "source": [
        "labtest(tokenize_rulebased)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80AWLZxgJKxr"
      },
      "source": [
        "Great. You could now tokenize and split out punctuation and numbers. There are a lot more rules that we could implement. Things like splitting \"don't\" into \"do\" and \"n't\" are common. But we'll stop here with our `tokenize_rulebased` function.\n",
        "\n",
        "### Stemming\n",
        "\n",
        "Now, let's have a look at the next step, which is normalizing some of the words.\n",
        "\n",
        "**Exercise:** Write a function (`stem_onerule`) that takes a list of tokens and trims off \"ing\" of any tokens that end with that suffix.\n",
        "\n",
        "For an input of `[\"He\", \"is\", \"reading\"]`, it should give an output of `[\"He\", \"is\", \"read\"]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgueQAx4LB8p"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPxsXZ3yL4ij"
      },
      "outputs": [],
      "source": [
        "labtest(stem_onerule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMg1_LOGMobU"
      },
      "source": [
        "Excellent. There are a lot of rules that can be implemented. The Porter Stemmer shown in class has a very long list of possible rules. We'll implement a couple more.\n",
        "\n",
        "**Exercise:** Extend your function above with the stemming rules below and complete `stem_morerules`. Note that one of the rules is that the word \"is\" shouldn't be changed.\n",
        "\n",
        "```\n",
        "is -> is\n",
        "(stem)ing -> stem\n",
        "(stem)ed -> stem\n",
        "(stem)s -> stem\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLpltcyLNF9K"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD07cPmttgon"
      },
      "outputs": [],
      "source": [
        "labtest(stem_morerules)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAC8wmGoKNLt"
      },
      "source": [
        "### Text Case\n",
        "\n",
        "Another small step often needed is converting all text to lowercase so that analyses become case-insensitive.\n",
        "\n",
        "**Exercise:** Implement the `lowercase_tokens` function that takes in a list of tokens and returns their lowercase version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJtTRUSHFNZ9"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWp52OZ_3fM1"
      },
      "outputs": [],
      "source": [
        "labtest(lowercase_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYWSRJQaSwlv"
      },
      "source": [
        "### Stopwords Removal\n",
        "\n",
        "We should also remove any common words from our list of tokens. There is no definitive list of stopwords. For now, we'll use the set proposed by C.J. Van Rijsbergen in his book Information Retrieval. The code below sets up the `stopwords` set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B43PJ4j5OmVC"
      },
      "outputs": [],
      "source": [
        "# Source is Table 2.1 of Chapter 2 of Information Retrieval by C.J. Van Rijsbergen\n",
        "# http://www.dcs.gla.ac.uk/Keith/Chapter.2/Ch.2.html\n",
        "\n",
        "stopword_text = \"\"\"\n",
        "A               CANNOT          INTO            OUR             THUS\n",
        "ABOUT           CO              IS              OURS            TO\n",
        "ABOVE           COULD           IT              OURSELVES       TOGETHER\n",
        "ACROSS          DOWN            ITS             OUT             TOO\n",
        "AFTER           DURING          ITSELF          OVER            TOWARD\n",
        "AFTERWARDS      EACH            LAST            OWN             TOWARDS\n",
        "AGAIN           EG              LATTER          PER             UNDER\n",
        "AGAINST         EITHER          LATTERLY        PERHAPS         UNTIL\n",
        "ALL             ELSE            LEAST           RATHER          UP\n",
        "ALMOST          ELSEWHERE       LESS            SAME            UPON\n",
        "ALONE           ENOUGH          LTD             SEEM            US\n",
        "ALONG           ETC             MANY            SEEMED          VERY\n",
        "ALREADY         EVEN            MAY             SEEMING         VIA\n",
        "ALSO            EVER            ME              SEEMS           WAS\n",
        "ALTHOUGH        EVERY           MEANWHILE       SEVERAL         WE\n",
        "ALWAYS          EVERYONE        MIGHT           SHE             WELL\n",
        "AMONG           EVERYTHING      MORE            SHOULD          WERE\n",
        "AMONGST         EVERYWHERE      MOREOVER        SINCE           WHAT\n",
        "AN              EXCEPT          MOST            SO              WHATEVER\n",
        "AND             FEW             MOSTLY          SOME            WHEN\n",
        "ANOTHER         FIRST           MUCH            SOMEHOW         WHENCE\n",
        "ANY             FOR             MUST            SOMEONE         WHENEVER\n",
        "ANYHOW          FORMER          MY              SOMETHING       WHERE\n",
        "ANYONE          FORMERLY        MYSELF          SOMETIME        WHEREAFTER\n",
        "ANYTHING        FROM            NAMELY          SOMETIMES       WHEREAS\n",
        "ANYWHERE        FURTHER         NEITHER         SOMEWHERE       WHEREBY\n",
        "ARE             HAD             NEVER           STILL           WHEREIN\n",
        "AROUND          HAS             NEVERTHELESS    SUCH            WHEREUPON\n",
        "AS              HAVE            NEXT            THAN            WHEREVER\n",
        "AT              HE              NO              THAT            WHETHER\n",
        "BE              HENCE           NOBODY          THE             WHITHER\n",
        "BECAME          HER             NONE            THEIR           WHICH\n",
        "BECAUSE         HERE            NOONE           THEM            WHILE\n",
        "BECOME          HEREAFTER       NOR             THEMSELVES      WHO\n",
        "BECOMES         HEREBY          NOT             THEN            WHOEVER\n",
        "BECOMING        HEREIN          NOTHING         THENCE          WHOLE\n",
        "BEEN            HEREUPON        NOW             THERE           WHOM\n",
        "BEFORE          HERS            NOWHERE         THEREAFTER      WHOSE\n",
        "BEFOREHAND      HERSELF         OF              THEREBY         WHY\n",
        "BEHIND          HIM             OFF             THEREFORE       WILL\n",
        "BEING           HIMSELF         OFTEN           THEREIN         WITH\n",
        "BELOW           HIS             ON              THEREUPON       WITHIN\n",
        "BESIDE          HOW             ONCE            THESE           WITHOUT\n",
        "BESIDES         HOWEVER         ONE             THEY            WOULD\n",
        "BETWEEN         I               ONLY            THIS            YET\n",
        "BEYOND          IE              ONTO            THOSE           YOU\n",
        "BOTH            IF              OR              THOUGH          YOUR\n",
        "BUT             IN              OTHER           THROUGH         YOURS\n",
        "BY              INC             OTHERS          THROUGHOUT      YOURSELF\n",
        "CAN             INDEED          OTHERWISE       THRU            YOURSELVES\n",
        "\"\"\"\n",
        "\n",
        "stopwords = set( [ s.lower().strip() for s in stopword_text.strip().split() ] )\n",
        "print(stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fldTZi_oTgNz"
      },
      "source": [
        "**Exercise:** Complete the `remove_stopwords` function that takes a list of tokens and returns a list of those not in the `stopwords` set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6qnr85Asu1S"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i-HMpBl3b2p"
      },
      "outputs": [],
      "source": [
        "labtest(remove_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNTbAQjyO9hs"
      },
      "source": [
        "### Combined Text Pipeline\n",
        "\n",
        "Let's put together our pipeline into a single function that calls all the methods we've already built. It should do the following\n",
        "\n",
        "- Tokenize with some of rules about punctuation\n",
        "- Stem the words with our small set of rules\n",
        "- Lowercase all the tokens\n",
        "- Remove stopwords\n",
        "\n",
        "The `text_pipeline` function below uses the previously defined functions to do that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6t23mH7PGeX"
      },
      "outputs": [],
      "source": [
        "def text_pipeline(text):\n",
        "  tokens = tokenize_rulebased(text)\n",
        "  stemmed = stem_morerules(tokens)\n",
        "  lowercased = lowercase_tokens(stemmed)\n",
        "  nostopwords = remove_stopwords(lowercased)\n",
        "  return nostopwords\n",
        "\n",
        "# Example usage:\n",
        "text_pipeline(\"The quick brown fox jumped over the lazy dog.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ7Peqy-REcN"
      },
      "source": [
        "We could obviously do more to improve the tokenization and stemming by adding more rules. We could also remove tokens that are punctuation as they're likely not useful either.\n",
        "\n",
        "## spaCy\n",
        "\n",
        "Now that we know how the individual components work, we're going to try out one of the popular text processing libraries: [spaCy](https://spacy.io/).\n",
        "\n",
        "If you run this lab locally or want to use spaCy outside Colab, you need to [install it](https://spacy.io/usage) with a language model (e.g. English).\n",
        "\n",
        "Before we start, we need to load an English language model in spaCy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm74n5ghRdrj"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJcncHUPRtIv"
      },
      "source": [
        "By default, spaCy will do all the functionality we need to tokenize, stem (& lemmatize), filter stopwords, punctuation and more. To use it, you invoke the `nlp` object created in the previous cell and call it with the text you want to process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fZAG037R3s5"
      },
      "outputs": [],
      "source": [
        "doc = nlp(\"Colorless green ideas sleep furiously.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sKT7FZ-R8Bd"
      },
      "source": [
        "You can then iterate through the result (`doc` in this case) to examine each of the resulting tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUvdQjLzJjN_"
      },
      "outputs": [],
      "source": [
        "for token in doc:\n",
        "  print(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAAKRBKfSmZ_"
      },
      "source": [
        "Each token is not a string. It's an object of a Token class. The [spaCy documentation](https://spacy.io/api/token#attributes) provides a lot of information about the properties that tokens have, including whether they are a stopword, are punctuation, their lemmatized form, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rD-hvYeKyor"
      },
      "outputs": [],
      "source": [
        "token = doc[0]\n",
        "type(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnjevUhvfB2b"
      },
      "source": [
        "You can access lots of useful things about each token. The most basic is the original text of the token: `token.text`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vWZobWRfBQG"
      },
      "outputs": [],
      "source": [
        "token.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPhtI7vU6Kx9"
      },
      "source": [
        "There's a lot more in the Token that we will use. Below are some examples. More information is available in the [spaCy documentation](https://spacy.io/api/token#attributes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR9lxKmV6aPE"
      },
      "outputs": [],
      "source": [
        "token = doc[2]\n",
        "print('Offset of token in source text:', token.idx)\n",
        "print('Source text of token:', token.text)\n",
        "print('Part of speech (e.g. noun, verb, etc):', token.pos_)\n",
        "print('Lemma (shortened/standardized form of word):', token.lemma_)\n",
        "print('Is this token a stopword (a common word which can be ignored)?:', token.is_stop)\n",
        "print('Is this token punctuation?:', token.is_punct)\n",
        "print('Is this token whitespace (e.g. spaces, tabs, newlines, etc)?:', token.is_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbHjys5SS_jf"
      },
      "source": [
        "Note that some of the properties of a token end with an underscore. Without the underscore, you get the numerical identifier of that part-of-speech or lemma. To get the text version, you need to use the name with an underscore.\n",
        "\n",
        "**Exercise:** Create a function (`tokenize_spacy`) that uses spaCy to tokenize the text and returns a list of the text of tokens. This means that you will need `token.text` and not just `token`. Look above for how `nlp` is used and what it returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vtb89YiCTlZ1"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KboMRsv4AhXH"
      },
      "outputs": [],
      "source": [
        "labtest(tokenize_spacy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeKwpHIP7Xju"
      },
      "source": [
        "As we saw above, spaCy provides more than the source text for each token. Let's use that information for our entire text pipeline in a single function.\n",
        "\n",
        "**Exercise:** Write a function `text_pipeline_spacy` that parses the input text with spaCy and provides the **lowercase** forms of the **lemmas** of the tokens if they are:\n",
        "- not stopwords\n",
        "- not punctuation\n",
        "- not whitespace\n",
        "\n",
        "Note that we want the **lemmas**. These are similar to stems, but the algorithm to get them considers the context in which the word appears. For example, this means that nouns and verbs are treated differently.\n",
        "\n",
        "See above for the information contained in the Tokens that `nlp` gives you. You can also check the [spaCy documentation](https://spacy.io/api/token#attributes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3bvCtJmTrjh"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P_884o_76fB"
      },
      "outputs": [],
      "source": [
        "labtest(text_pipeline_spacy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0OIm4zL8Yed"
      },
      "source": [
        "## Back to the Reddit Data\n",
        "\n",
        "After all that, we are finally getting back to the Reddit data. Recall that each post contains text in the `title` and `body`. When we process them, we're going to concatenate them and use our spaCy pipeline to get the filtered tokens for us. First, let's recall what a post looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY0c5ahX8m_j"
      },
      "outputs": [],
      "source": [
        "posts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix-q3SNW8mOJ"
      },
      "source": [
        "Let's use the spaCy pipeline function to get tokens for all the Reddit posts.  This may take a minute. We're going to use the [tqdm](https://tqdm.github.io/) library to get some nice information about the runtime of our code. You can use it with a for loop as below. Check the [website](https://tqdm.github.io/) for some more nice examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxbjIDWe8ihK"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm # This provides a nice progress bar\n",
        "\n",
        "for post in tqdm(posts):\n",
        "  post['tokens'] = text_pipeline_spacy(post['title'] + '\\n' + post['body'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuVoMuWJ90-J"
      },
      "source": [
        "Initially, we want to look at our target post and the two options (A and B) shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLFzaxYh9htI"
      },
      "outputs": [],
      "source": [
        "target_post = posts[0]\n",
        "option_a = posts[1]\n",
        "option_b = posts[2]\n",
        "\n",
        "print('TARGET:',target_post['title'])\n",
        "print(target_post['body'])\n",
        "print('-'*30)\n",
        "print('A:', option_a['title'])\n",
        "print(option_a['body'])\n",
        "print('-'*30)\n",
        "print('B:', option_b['title'])\n",
        "print(option_b['body'])\n",
        "print('-'*30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1PUF_j3-YPn"
      },
      "source": [
        "By using `text_pipeline_spacy` above, we have the tokens for all the posts. Hence, we have the tokens for our target post and the two options here. We can start measuring the overlap of tokens with these lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flxt7iU890PG"
      },
      "outputs": [],
      "source": [
        "print('TARGET:', target_post['tokens'])\n",
        "print()\n",
        "print('A:', option_a['tokens'])\n",
        "print()\n",
        "print('B:', option_b['tokens'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3qZMn77AiqR"
      },
      "source": [
        "We noted that the number of overlapping words may be valuable signal if two documents talk about similar things.\n",
        "\n",
        "**Exercise:** Complete the `count_overlapping_tokens` function that takes two lists of tokens and returns the number of **unique** tokens shared between the two lists. You could implement this by iterating over one group of unique tokens and checking if each one is in the other group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT8DORE6-Sje"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdEjlDKsAeR2"
      },
      "outputs": [],
      "source": [
        "labtest(count_overlapping_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuRJEatQMViG"
      },
      "source": [
        "The Python [set class](https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset) provides some useful functionality, including `intersection`, `union` and `difference` functions. These are mentioned in the [Python docs for set](https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset).\n",
        "\n",
        "**Exercise:** Write the function `count_overlapping_tokens_with_sets` using the functionality of the set class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVKhR4A2AhVT"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe7E7VynOR1g"
      },
      "outputs": [],
      "source": [
        "labtest(count_overlapping_tokens_with_sets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmpFu14KO_7T"
      },
      "source": [
        "Now let's implement one of the similarity measures that we discussed in lecture.\n",
        "\n",
        "**Exercise:** Implement the `jaccard_similarity` function using the equation below.\n",
        "\n",
        "Recall that `X ∩ Y` is the intersection of the sets X and Y, `X ∪ Y` is their union and `| ... |` is the number of elements in the given set.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq3HlvtqQT9M"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5uO1MQ9Qd6R"
      },
      "outputs": [],
      "source": [
        "labtest(jaccard_similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox96UEfaQkiR"
      },
      "source": [
        "Great. Now we can use a measure to compare our original target post and the two options. Let's use `jaccard_similarity`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjnlhpY8QiDA"
      },
      "outputs": [],
      "source": [
        "jaccard_similarity(target_post['tokens'], option_a['tokens'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrYBDKE4QwWC"
      },
      "outputs": [],
      "source": [
        "jaccard_similarity(target_post['tokens'], option_b['tokens'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj6U5RpGRAA6"
      },
      "source": [
        "Option A wins!\n",
        "\n",
        "However, the similarity scores are fairly low. It looks like there are too many other words that don't overlap between these documents. Maybe there are some documents that provide a higher score.\n",
        "\n",
        "The code below calculates the similarity scores using `jaccard_similarity` for all posts against our `target_post`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToUDjhVRQ2Zr"
      },
      "outputs": [],
      "source": [
        "scores_with_posts = []\n",
        "for post in posts:\n",
        "  similarity_score = jaccard_similarity(target_post['tokens'], post['tokens'])\n",
        "  scores_with_posts.append( (similarity_score, post))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUjhTHeLZrdX"
      },
      "source": [
        "We then need to sort the posts. We want them in descending order, so use `reverse=True`. We also want to sort by the score only, so give `key=lambda x:x[0]`, which tells it to extract the first element from each tuple (the similarity_score) and use that for sorting. If we didn't do that, it would try to sort the posts, which are dictionaries, and you can't directly sort dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4fML-0_ZrtJ"
      },
      "outputs": [],
      "source": [
        "sorted_scores_with_posts = sorted(scores_with_posts, reverse=True, key=lambda x:x[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbyjplOWaUho"
      },
      "source": [
        "We'll print out the top 10 closest posts below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57NO0_h3TP7Y"
      },
      "outputs": [],
      "source": [
        "for score,post in sorted_scores_with_posts[:10]:\n",
        "  print(f\"{score:.2f}\\t{post['title']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZUFZj0dY7i5"
      },
      "source": [
        "The closest post is the target post itself (which is always a good sanity check). Then there are some other posts about drinks. This looks pretty good. The dataset also contains posts on videogames and other topics and they don't appear. So it's broadly working!\n",
        "\n",
        "It's good to think about the strengths and weaknesses of this set-based approach. There are much more complicated methods for document similarity so these methods are more straightforward. But they also ignore important things like word frequencies and word importance.\n",
        "\n",
        "## Wrap Up\n",
        "\n",
        "Some final **open questions** to think about. **There are no definitive answers to these**\n",
        "\n",
        "1. What extra steps could you add to this set-based approach to improve the similarity scores?\n",
        "2. How could you measure which similarity measure works best for this set of documents?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End\n",
        "\n",
        "This is the end of Lab 1. Let us know if you encounter any issues! Remember we are here to help!\n",
        "\n",
        "In this lab, you...\n",
        "- learned about the labtest function\n",
        "- examine the similarity problem from the lecture\n",
        "- build a tokenizer to split up text into tokens\n",
        "- coded a stemmer to remove the stems from words\n",
        "- combined these with lowercasing and stopword removal to create a full pipeline\n",
        "- learned how spaCy can do these steps\n",
        "\n",
        "**Please submit your lab through Moodle. We don't mark the labs but it helps us to craft better labs in the future**"
      ],
      "metadata": {
        "id": "BWjJ268TYL-C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He7PbddegWs3"
      },
      "source": [
        "\n",
        "## Optional Extras\n",
        "\n",
        "Below is an entirely optional extra idea if you want to dig into a topic more deeply.\n",
        "\n",
        "**Regular Expressions for Stemming:** Instead of using `if` statements and some manual text manipulation to define the rules for stemming, you could implement them using regular expressions. You would likely want to use the [re.sub](https://docs.python.org/3/library/re.html#re.sub) function. Can you define a function `stem_with_regex` that implements the same rules as `stem_morerules` but with a set of regular expressions? Or one regular expression?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}