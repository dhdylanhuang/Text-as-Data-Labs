{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhdylanhuang/Text-as-Data-Labs/blob/main/Lab_2_Geometric_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text as Data Lab 2: Geometric similarity\n",
        "\n",
        "The goal of the first lab is to discover several ways to represent documents as numerical vectors and how they can be used for calculating the similarity of our Reddit posts.\n",
        "\n",
        "**Before you start, save a copy of this lab to your drive using \"File > Save a Copy in Drive\".** If you skip this step, you may lose progress that you have made (e.g., if you close the browser tab or your computer crashes).\n",
        "\n",
        "This lab is linked with the second lecture. Please watch the lecture and refer to the slides as needed.\n",
        "\n",
        "In this lab you will learn about:\n",
        "- Representing documents with one-hot encoding, term frequency and TFIDF\n",
        "- Storing vectors using dense and sparse representations\n",
        "- Zipf's law\n",
        "- Using the vectors to calculate similarity"
      ],
      "metadata": {
        "id": "QtJndOXXv2BM"
      },
      "id": "QtJndOXXv2BM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Colab may hide some of this lab by collapsing a section. You'd see something that says \"X cells hidden\" (like below). Click on it to expand that section of the lab.\n"
      ],
      "metadata": {
        "id": "u0n0NPYDyBB8"
      },
      "id": "u0n0NPYDyBB8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "VokVMt2ZyJgJ"
      },
      "id": "VokVMt2ZyJgJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the labtest function\n",
        "\n",
        "As with the first lab, we'll use a `labtest` function so that you can check your code. You need to install and load it with the code below."
      ],
      "metadata": {
        "id": "iyRL3KbzupsJ"
      },
      "id": "iyRL3KbzupsJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Installs the labtest system and loads the tests for this specific lab\n",
        "\n",
        "!pip install -U git+https://github.com/jakelever/glasgowcs_labtest.git\n",
        "from glasgowcs_labtest.textasdata.lab2 import labtest"
      ],
      "metadata": {
        "id": "zDOMAwmQwZJ5"
      },
      "id": "zDOMAwmQwZJ5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Reddit data\n",
        "\n",
        "We'll be using the same Reddit data from the previous labs. We will download it and tokenize it with [spaCy](https://spacy.io/) for later use.\n",
        "\n",
        "First, let's download it:"
      ],
      "metadata": {
        "id": "fApkt8zXu35a"
      },
      "id": "fApkt8zXu35a"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O reddit_posts.json https://gla-my.sharepoint.com/:u:/g/personal/jake_lever_glasgow_ac_uk/EY_R8Y7DkrxMqXGe-zlgeNkBdJU5ZNTf8FYrN2pqDwddMA?download=1"
      ],
      "metadata": {
        "id": "ehQpMnH5wwYM"
      },
      "id": "ehQpMnH5wwYM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load in the posts:"
      ],
      "metadata": {
        "id": "PvvQ72A5vnts"
      },
      "id": "PvvQ72A5vnts"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7bc1fcf",
      "metadata": {
        "id": "a7bc1fcf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('reddit_posts.json') as f:\n",
        "    posts = json.load(f)\n",
        "\n",
        "len(posts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's load spaCy."
      ],
      "metadata": {
        "id": "0dA5WFmWvf7N"
      },
      "id": "0dA5WFmWvf7N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "601da9dc",
      "metadata": {
        "id": "601da9dc"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we'll use a function that we developed last lab to tokenize the posts."
      ],
      "metadata": {
        "id": "nS7sRLzMvhxi"
      },
      "id": "nS7sRLzMvhxi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "574d351a",
      "metadata": {
        "id": "574d351a"
      },
      "outputs": [],
      "source": [
        "def text_pipeline_spacy(text):\n",
        "    tokens = []\n",
        "    doc = nlp(text)\n",
        "    for t in doc:\n",
        "        if not t.is_stop and not t.is_punct and not t.is_space:\n",
        "            tokens.append(t.lemma_.lower())\n",
        "    return tokens\n",
        "\n",
        "# Example usage:\n",
        "text_pipeline_spacy(\"Of all the things I miss, I miss my mind the most.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's run it across all the posts to tokenize it. Recall that we can use the [tqdm library](https://tqdm.github.io/) to give us a nice progress bar."
      ],
      "metadata": {
        "id": "uvX7dt9OvySQ"
      },
      "id": "uvX7dt9OvySQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad04d13f",
      "metadata": {
        "id": "ad04d13f"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm # This provides a nice progress bar\n",
        "\n",
        "for post in tqdm(posts):\n",
        "    post['tokens'] = text_pipeline_spacy(post['title'] + '\\n' + post['body'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the full Reddit posts later on in the lab."
      ],
      "metadata": {
        "id": "ktpElOcTwDFK"
      },
      "id": "ktpElOcTwDFK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector representation\n",
        "\n",
        "In this lab, we're going to learn about another representation of documents. Last lab, we used [sets](https://docs.python.org/3/library/stdtypes.html#set) to represent the unique tokens of a document. This lab, we're going to use numerical vectors to represent documents.\n",
        "\n",
        "Before we get started, here is a mini dataset that we'll use for a few of our functions. This is instead of running across the entire Reddit dataset for every function. We'll use the `text_pipeline_spacy` function from above to tokenize the sentences."
      ],
      "metadata": {
        "id": "hbnL0O95KGui"
      },
      "id": "hbnL0O95KGui"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7752d456",
      "metadata": {
        "id": "7752d456"
      },
      "outputs": [],
      "source": [
        "mini_sentences = [\n",
        "    \"My favourite soft drink is Apple Tango, but I also love Irn Bru.\",\n",
        "    \"Irn Bru is a great drink.\",\n",
        "    \"I once found a can of Irn Bru in St Petersburg.\",\n",
        "    \"Irn Bru is a soft drink launched in 1901 by AG Barr.\",\n",
        "    \"IRN-BRU is made at AG Barr in Cumbernauld.\"\n",
        "]\n",
        "\n",
        "mini_tokenized = [ text_pipeline_spacy(x) for x in mini_sentences ]\n",
        "\n",
        "mini_tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building our own text vectorizer\n",
        "\n",
        "There are a few components to a vectorizer including collecting the vocabulary of a set of documents and several different ways to turn a document into a vector, knowing that vocabulary. We're going to build this up from scratch so you understand all the parts."
      ],
      "metadata": {
        "id": "lDDrYBEHyw1r"
      },
      "id": "lDDrYBEHyw1r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The vocabulary\n",
        "\n",
        "The first thing is to collect all the unique tokens in a corpus (collection of documents). And each unique token should be mapped to an integer ID, which should start from zero.\n",
        "\n",
        "**Exercise:** Write a function `make_vocabulary` that takes a corpus (i.e. a list of lists of tokens like `mini_tokenized` above) and returns a dictionary with each unique token mapped to an integer, starting from zero. **The order of the tokens does not matter.**\n",
        "\n",
        "For example, the output of `make_vocabulary( [ ['irn','bru','good'], ['irn','bru','bad'] ])` should be `{'bad': 0, 'bru': 1, 'good': 2, 'irn': 3}`. Recall that dictionaries can be created with `{}` just as lists are made with `[]`."
      ],
      "metadata": {
        "id": "uViV30mjy8bJ"
      },
      "id": "uViV30mjy8bJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b93414c",
      "metadata": {
        "id": "6b93414c"
      },
      "outputs": [],
      "source": [
        "def make_vocabulary(corpus):\n",
        "  # your code!\n",
        "\n",
        "# Example usage:\n",
        "make_vocabulary( [ ['irn','bru','good'], ['irn','bru','bad'] ])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that you can test your code with the `labtest` function:"
      ],
      "metadata": {
        "id": "eNRFN48hxu-9"
      },
      "id": "eNRFN48hxu-9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b0af834",
      "metadata": {
        "id": "8b0af834"
      },
      "outputs": [],
      "source": [
        "labtest(make_vocabulary)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run it on our mini corpus to see what it happens."
      ],
      "metadata": {
        "id": "dtgcXLlMxyn_"
      },
      "id": "dtgcXLlMxyn_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8d4f11b",
      "metadata": {
        "id": "f8d4f11b"
      },
      "outputs": [],
      "source": [
        "make_vocabulary(mini_tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dense vector representation\n",
        "\n",
        "Now we have our vocabulary, we can start making vectors. We'll start off with a dense one-hot vector. For a vocabulary with `N` unique tokens, a dense vector will contain `N` elements. The one-hot vector means that it will be all zeros except for the elements corresponding to the tokens in that document which are one.\n",
        "\n",
        "**Exercise:** Write a function `make_onehot_dense` that takes in the tokens of a document and the vocab dictionary and returns a one-hot vector (as a list) for that document."
      ],
      "metadata": {
        "id": "ULkHOV2LJv_S"
      },
      "id": "ULkHOV2LJv_S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "018463fc",
      "metadata": {
        "id": "018463fc"
      },
      "outputs": [],
      "source": [
        "def make_onehot_dense(tokens, vocab):\n",
        "  # your code!\n",
        "\n",
        "# Example usage:\n",
        "make_onehot_dense(['irn','bru','good','good'], {'irn':0,'bru':1,'good':2,'bad':3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba1d2d31",
      "metadata": {
        "id": "ba1d2d31"
      },
      "outputs": [],
      "source": [
        "labtest(make_onehot_dense)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can run it on the mini corpus. You can use the mini corpus later on too if it's helpful to debug your code."
      ],
      "metadata": {
        "id": "mTSsup6Iy0le"
      },
      "id": "mTSsup6Iy0le"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4fe9b2a",
      "metadata": {
        "id": "d4fe9b2a"
      },
      "outputs": [],
      "source": [
        "mini_vocab = make_vocabulary(mini_tokenized)\n",
        "make_onehot_dense(mini_tokenized[0],mini_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sparse vector representation\n",
        "\n",
        "Dense vectors have a big problem. They take up a lot of memory, even when containing only a few non-zero values. We can use a sparse representation that only stores the non-zero data.\n",
        "\n",
        "Remember that a sparse representation is generally a good idea for sparse data (mostly zeros), and a dense representation (like above) is a good idea for dense data (mostly non-zeros). If you use the wrong one for the wrong data, you'll have a bad time.\n",
        "\n",
        "We used a `list` to represent our dense vector as below."
      ],
      "metadata": {
        "id": "mQlIYtvuJsqJ"
      },
      "id": "mQlIYtvuJsqJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a231ab",
      "metadata": {
        "id": "82a231ab"
      },
      "outputs": [],
      "source": [
        "dense_vector = [1,0,1,0,1]\n",
        "type(dense_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the lecture, sparse data was shown as a list of pairs with the first number being the element index of the vector and the second element being the value in that element.\n",
        "\n",
        "For practical reasons, we're going to use a `dict` to represent our sparse vector and not a list of pairs of numbers. This allows a quick look up of values in a sparse vector.\n",
        "\n",
        "Below is a sparse vector that contains the same data as the `dense_vector` variable above."
      ],
      "metadata": {
        "id": "pTYeozWhzniX"
      },
      "id": "pTYeozWhzniX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bca1172",
      "metadata": {
        "id": "5bca1172"
      },
      "outputs": [],
      "source": [
        "sparse_vector = {0:1, 2:1, 4:1}\n",
        "type(sparse_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two useful things to know about dictionaries.\n",
        "\n",
        "1. You can give a second argument to the dictionary's [.get method](https://docs.python.org/3/library/stdtypes.html#dict.get) and if the dictionary doesn't contain it, it will return that value instead. For instance, the `sparse_vector` above doesn't contain a value at element 1:"
      ],
      "metadata": {
        "id": "RxaNZxVE0Nct"
      },
      "id": "RxaNZxVE0Nct"
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_vector.get(1,0)"
      ],
      "metadata": {
        "id": "VGUGRas60a_t"
      },
      "id": "VGUGRas60a_t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Dictionaries have a function called `items()` that allows you to iterate over the keys and values in the dictionary:"
      ],
      "metadata": {
        "id": "Kq7Q057g0goW"
      },
      "id": "Kq7Q057g0goW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92f272c0",
      "metadata": {
        "id": "92f272c0"
      },
      "outputs": [],
      "source": [
        "# Introducing .items()\n",
        "for key,val in sparse_vector.items():\n",
        "  print(key, val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Write a function `sparse_to_dense` that converts a sparse vector (stored as a `dict`) to a dense vector (stored as a `list`). In this case, you also need to give the length of the vector: `vector_length`."
      ],
      "metadata": {
        "id": "FD6d7Byf1NiK"
      },
      "id": "FD6d7Byf1NiK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dbdef40",
      "metadata": {
        "id": "3dbdef40"
      },
      "outputs": [],
      "source": [
        "def sparse_to_dense(sv, vector_length):\n",
        "  # your code!\n",
        "\n",
        "# Example usage:\n",
        "sparse_to_dense({0:1, 2:1, 4:1}, 5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labtest(sparse_to_dense)"
      ],
      "metadata": {
        "id": "OcC4bjfs1OeF"
      },
      "id": "OcC4bjfs1OeF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you can see the format of sparse vectors, we'll keep using them from now on. Dense vectors have some serious memory problems as we'll explore so it's good to stick with sparse vectors. Now it's time to rewrite the one-hot vector function with sparse vectors.\n",
        "\n",
        "**Exercise**: Write a function `make_onehot_sparse` that gives a one-hot vector like above but returns a sparse vector.\n",
        "\n",
        "For performance reasons, you shouldn't create a dense vector and then convert it, or this lab will run very slowly."
      ],
      "metadata": {
        "id": "TRG9b_EU3zzj"
      },
      "id": "TRG9b_EU3zzj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5146067d",
      "metadata": {
        "id": "5146067d"
      },
      "outputs": [],
      "source": [
        "def make_onehot_sparse(tokens, vocab):\n",
        "  # your code!\n",
        "\n",
        "# Example usage:\n",
        "make_onehot_sparse(['irn','bru','good','good'], {'irn':0,'bru':1,'good':2,'bad':3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "764bb2ee",
      "metadata": {
        "id": "764bb2ee"
      },
      "outputs": [],
      "source": [
        "labtest(make_onehot_sparse)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Memory usage of dense & sparse vectors\n",
        "\n",
        "Let's explore the performance of dense versus sparse vectors with the Reddit data set. First, let's create a vocabulary. We need to pull out the tokens from each post to give to the `make_vocabulary` function."
      ],
      "metadata": {
        "id": "u1Yq2l7IJlkZ"
      },
      "id": "u1Yq2l7IJlkZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9010d996",
      "metadata": {
        "id": "9010d996"
      },
      "outputs": [],
      "source": [
        "post_tokens = [ p['tokens'] for p in posts ]\n",
        "posts_vocab = make_vocabulary(post_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll create dense and sparse vectors for every post"
      ],
      "metadata": {
        "id": "eLQouK9r4lv1"
      },
      "id": "eLQouK9r4lv1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cd5fa25",
      "metadata": {
        "id": "1cd5fa25"
      },
      "outputs": [],
      "source": [
        "posts_dense = [ make_onehot_dense(p['tokens'], posts_vocab) for p in posts ]\n",
        "posts_sparse = [ make_onehot_sparse(p['tokens'], posts_vocab) for p in posts ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to check the memory requirements for each set of vectors. The `sys` module has a function `getsizeof`. However, it doesn't quite do what we want. As each set of vectors is a nested structure (e.g. a list of lists or a list of dicts), the `getsizeof` function doesn't calculate it properly as below."
      ],
      "metadata": {
        "id": "P8J8mfM84xHW"
      },
      "id": "P8J8mfM84xHW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7263cbf5",
      "metadata": {
        "id": "7263cbf5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# This doesn't work\n",
        "print(f\"sys.getsizeof(posts_dense)={sys.getsizeof(posts_dense)}\")\n",
        "print(f\"sys.getsizeof(posts_sparse)={sys.getsizeof(posts_sparse)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's in bytes. They should take up more space than that!\n",
        "\n",
        "Instead, we go to good ol' Stack Overflow as a nice person has written a recursive function that can go inside objects to calculate the size. You do not need to understand how this works (but please do dig into it if it intrigues you)."
      ],
      "metadata": {
        "id": "YWOyU2mL5Hi8"
      },
      "id": "YWOyU2mL5Hi8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca77f5e7",
      "metadata": {
        "id": "ca77f5e7"
      },
      "outputs": [],
      "source": [
        "# From https://stackoverflow.com/questions/449560/how-do-i-determine-the-size-of-an-object-in-python\n",
        "\n",
        "import sys\n",
        "from types import ModuleType, FunctionType\n",
        "from gc import get_referents\n",
        "\n",
        "# Custom objects know their class.\n",
        "# Function objects seem to know way too much, including modules.\n",
        "# Exclude modules as well.\n",
        "BLACKLIST = type, ModuleType, FunctionType\n",
        "\n",
        "def getsize(obj):\n",
        "    \"\"\"sum size of object & members.\"\"\"\n",
        "    if isinstance(obj, BLACKLIST):\n",
        "        raise TypeError('getsize() does not take argument of type: '+ str(type(obj)))\n",
        "    seen_ids = set()\n",
        "    size = 0\n",
        "    objects = [obj]\n",
        "    while objects:\n",
        "        need_referents = []\n",
        "        for obj in objects:\n",
        "            if not isinstance(obj, BLACKLIST) and id(obj) not in seen_ids:\n",
        "                seen_ids.add(id(obj))\n",
        "                size += sys.getsizeof(obj)\n",
        "                need_referents.append(obj)\n",
        "        objects = get_referents(*need_referents)\n",
        "    return size"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below runs it for both datasets and uses some nice F-string formatting. Check out [this page](https://www.w3schools.com/python/python_string_formatting.asp) for more details about that."
      ],
      "metadata": {
        "id": "gADaCtfK5V5s"
      },
      "id": "gADaCtfK5V5s"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{getsize(posts_dense)=:,}\")\n",
        "print(f\"{getsize(posts_sparse)=:,}\")"
      ],
      "metadata": {
        "id": "296dIv-P5Po6"
      },
      "id": "296dIv-P5Po6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow! The dense vectors do take up A LOT more memory than the sparse vectors. Let's delete the dense vectors (below) and Python will (eventually) clear the memory being used for them. We'll stick with the sparse vectors."
      ],
      "metadata": {
        "id": "RP5HxQVj5Zm-"
      },
      "id": "RP5HxQVj5Zm-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12495976",
      "metadata": {
        "id": "12495976"
      },
      "outputs": [],
      "source": [
        "del posts_dense"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Term frequency\n",
        "\n",
        "The one-hot vector only captured whether a token was in a document or not. It doesn't show how frequently the token occurred. We discussed in class that the frequency (term frequency) may be important too as tokens that occur many times may give a clue about the document's topics.\n",
        "\n",
        "Let's take a look at `Counter` first. It may be useful to you. It's a common Python data structure for counting things. You can create a Counter by giving it a list of things to count:"
      ],
      "metadata": {
        "id": "Vr-NZl2iJgPA"
      },
      "id": "Vr-NZl2iJgPA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93aec984",
      "metadata": {
        "id": "93aec984"
      },
      "outputs": [],
      "source": [
        "# Recall Counter\n",
        "from collections import Counter\n",
        "counts = Counter(['a','b','a','c'])\n",
        "counts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or create an empty `Counter` and increment the values of each element."
      ],
      "metadata": {
        "id": "YzRLiFx26EWz"
      },
      "id": "YzRLiFx26EWz"
    },
    {
      "cell_type": "code",
      "source": [
        "counts = Counter()\n",
        "counts['a'] += 1\n",
        "counts['b'] += 1\n",
        "counts['a'] += 1\n",
        "counts['c'] += 1\n",
        "counts"
      ],
      "metadata": {
        "id": "qC7_S4YP5uA1"
      },
      "id": "qC7_S4YP5uA1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Write a function `make_tf_sparse` that creates a vector representation of a document with term-frequency values. Instead of ones and zeros as in `make_onehot_sparse`, the non-zero values should be the term frequencies. You may want to use `Counter`."
      ],
      "metadata": {
        "id": "fp19uzmI6MDZ"
      },
      "id": "fp19uzmI6MDZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "041cc68e",
      "metadata": {
        "id": "041cc68e"
      },
      "outputs": [],
      "source": [
        "def make_tf_sparse(tokens, vocab):\n",
        "  # your code!\n",
        "\n",
        "# Example usage:\n",
        "make_tf_sparse(['irn','bru','good','good'], {'irn':0,'bru':1,'good':2,'bad':3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d450acfc",
      "metadata": {
        "id": "d450acfc"
      },
      "outputs": [],
      "source": [
        "labtest(make_tf_sparse)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TF-IDF: Term frequency - inverse document frequency\n",
        "\n",
        "The term frequency is certainly important, but maybe only important for some words. Some tokens are so common that they are expected to appear a lot. We've already filtered out many using the stopwords functionality of spaCy. But there will still be many unimportant words.\n",
        "\n",
        "Now we're going to look at TF-IDF which normalizes term frequency with the number of documents that a term appears in. This means that terms that appear across a lot of documents are deemed less important. But first we need to know the number of documents that each token appears in.\n",
        "\n",
        "**Exercise:** Write a function `doc_frequency` that takes a corpus (i.e. a list of token lists like `mini_tokenized` at the start) and returns a dictionary (or Counter) which maps each token to the number of documents it appears in.\n",
        "\n",
        "For example, the input `[['a','b'], ['b','c']]` should give the output: `{'a':1, 'b':2, 'c':1}`"
      ],
      "metadata": {
        "id": "zMuk5ROxJZGZ"
      },
      "id": "zMuk5ROxJZGZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35b8b47e",
      "metadata": {
        "id": "35b8b47e"
      },
      "outputs": [],
      "source": [
        "def doc_frequency(corpus):\n",
        "  # your code!\n",
        "\n",
        "# Example usage:\n",
        "doc_frequency( [ ['a','a','b'],\n",
        "                 ['a','b','b','c'],\n",
        "                 ['a','d','e','f'] ] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79ae33a8",
      "metadata": {
        "id": "79ae33a8"
      },
      "outputs": [],
      "source": [
        "labtest(doc_frequency)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can run it on the slightly larger set `mini_tokenized`."
      ],
      "metadata": {
        "id": "0JyEqWPY8UXJ"
      },
      "id": "0JyEqWPY8UXJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81139693",
      "metadata": {
        "id": "81139693"
      },
      "outputs": [],
      "source": [
        "doc_frequency(mini_tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we come to calculating TF-IDF. There are a few ways to calculate both the term frequency and inverse document frequency components.\n",
        "\n",
        "As discussed in class, it can be a good idea to use logarithms during these calculations so that very frequent words aren't weighted too highly.\n",
        "\n",
        "Refer to the slides in Lecture 2 for the equations to use for term frequency and inverse document frequency respectively. We'll use log 10 for both equations. And recall that TF-IDF is TF multiplied by IDF for each token.\n",
        "\n",
        "**Exercise:** Using the given definitions of TF and IDF, write a function  `make_tfidf_sparse` that returns the TF-IDF sparse vector for a document (given the list of tokens and the vocabulary mapping)."
      ],
      "metadata": {
        "id": "zXtSQoJE8cVz"
      },
      "id": "zXtSQoJE8cVz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745f57c9",
      "metadata": {
        "id": "745f57c9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def make_tfidf_sparse(tokens, vocab, doc_freq, N):\n",
        "  # your code!\n",
        "\n",
        "# Example usage:\n",
        "make_tfidf_sparse(['irn','bru','good','good'],\n",
        "                  {'irn':0,'bru':1,'good':2,'bad':3},\n",
        "                  {'irn':4,'bru':4,'good':7,'bad':9},\n",
        "                  10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da6edda",
      "metadata": {
        "id": "4da6edda"
      },
      "outputs": [],
      "source": [
        "labtest(make_tfidf_sparse)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using your vectorizer with a new corpus\n",
        "\n",
        "There are plenty of scenarios where you may want to vectorize new documents without starting from scratch and redefining your vocabulary:\n",
        "\n",
        "- It may have been computationally expensive and you just want to add some more document vectors to your existing collection and don't care about any novel tokens\n",
        "- You may be splitting your data into training and test sets for a machine learning experiment. You could want to vectorize your training set and test set separately but in the same way, using the vocabulary of your training set only.\n",
        "\n",
        "Let's create some vectorizer functions that are happy to work with new text. The big hurdle is how to deal with tokens which aren't in your original vocabulary. There are two approaches:\n",
        "\n",
        "1. Ignore them - very popular and actually makes a lot of sense. If you've never seen them before, you may not know what to do with them\n",
        "2. Treat them as a special token `<UNK>` - may be useful for certain applications to know that there are new tokens in your text\n",
        "\n",
        "Let's start with the first option. We'll do one-hot encoding as the functions will be simpler.\n",
        "\n",
        "**Exercise:** Write a function `make_onehot_ignorenewtokens` which works like `make_onehot_sparse` and ignores any tokens that aren't in the vocabulary."
      ],
      "metadata": {
        "id": "AejEN60kRlcR"
      },
      "id": "AejEN60kRlcR"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_onehot_ignorenewtokens(tokens, vocab):\n",
        "  # your code!\n",
        "\n",
        "# Example usage:\n",
        "make_onehot_ignorenewtokens(['irn','bru','fantastic'], {'irn':0,'bru':1,'good':2,'bad':3})"
      ],
      "metadata": {
        "id": "YtyGM1sjS7w_"
      },
      "id": "YtyGM1sjS7w_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labtest(make_onehot_ignorenewtokens)"
      ],
      "metadata": {
        "id": "Xx5bk62lTKmf"
      },
      "id": "Xx5bk62lTKmf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second option for dealing with new tokens is to add a special `<UNK>` token (which stands for unknown) to the vocabulary and map everything new to that.\n",
        "\n",
        "**Exercise:** Write a function `make_vocabulary_with_unk` (by slightly modifying `make_vocabulary`) that adds a `<UNK>` token to the vocabulary."
      ],
      "metadata": {
        "id": "agyUn76_TpPN"
      },
      "id": "agyUn76_TpPN"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_vocabulary_with_unk(corpus):\n",
        "  # your code!\n",
        "\n",
        "# Example usage:\n",
        "make_vocabulary_with_unk( [ ['irn','bru','good'], ['irn','bru','bad'] ])"
      ],
      "metadata": {
        "id": "8EHFBL7VT5jI"
      },
      "id": "8EHFBL7VT5jI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labtest(make_vocabulary_with_unk)"
      ],
      "metadata": {
        "id": "PCqRym3YTn5-"
      },
      "id": "PCqRym3YTn5-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, time to use our vocabulary with `<UNK>` to create a vector.\n",
        "\n",
        "**Exercise:** Write a `make_onehot_unk` function that create a sparse one hot encoding of the input tokens and maps any new tokens (not in the vocabulary) to the `<UNK>` token id.\n",
        "\n",
        "*Tip:* This is another great time to use a dictionary's `.get` method with the second argument as described earlier."
      ],
      "metadata": {
        "id": "UzxniFzfVHgS"
      },
      "id": "UzxniFzfVHgS"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_onehot_unk(tokens, vocab):\n",
        "  # your code!\n",
        "\n",
        "# Example usage:\n",
        "make_onehot_unk(['irn','bru','fantastic'], {'irn':0,'bru':1,'good':2,'bad':3,'<UNK>':4})"
      ],
      "metadata": {
        "id": "Luiiva2gVt5y"
      },
      "id": "Luiiva2gVt5y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labtest(make_onehot_unk)"
      ],
      "metadata": {
        "id": "W_UKKE0IBgA4"
      },
      "id": "W_UKKE0IBgA4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zipf's Law\n",
        "\n",
        "We'll take a little interlude to show why we used logarithms in the TFIDF equations. In basically any text corpus, there are a small number of very frequent tokens while most tokens appear fairly infrequently. This is known as [Zipf's law](https://en.wikipedia.org/wiki/Zipf's_law). Let's see if it's true in our Reddit data.\n",
        "\n",
        "First, let's get the counts for each token. This will look similar to your `doc_frequency` function above, but we're counting all mentions of tokens. So if it appears multiple times in a single document, every mention counts."
      ],
      "metadata": {
        "id": "ZTjbBC8EJR_y"
      },
      "id": "ZTjbBC8EJR_y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dd50a1c",
      "metadata": {
        "id": "3dd50a1c"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "token_counts = Counter()\n",
        "for post in posts:\n",
        "  for token in post['tokens']:\n",
        "    token_counts[token] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll order them and calculate their rank (whether they are first, second, third, etc in the list). The `most_common` function on a Counter is nice as it returns the Counter data in descending order. We use a list comprehension with the [enumerate function](https://docs.python.org/3/library/functions.html#enumerate) to help us calculate the rank."
      ],
      "metadata": {
        "id": "W_zklb9gB0cI"
      },
      "id": "W_zklb9gB0cI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df1db5f",
      "metadata": {
        "id": "7df1db5f"
      },
      "outputs": [],
      "source": [
        "token_counts_in_order = token_counts.most_common()\n",
        "token_counts_with_rank = [ (i+1,t,c) for i,(t,c) in enumerate(token_counts_in_order)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at the top ten tokens in the list. Remember that we have already removed stopwords (so things like \"the\" shouldn't appear)."
      ],
      "metadata": {
        "id": "aSczkj24CFvl"
      },
      "id": "aSczkj24CFvl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3eb0a25",
      "metadata": {
        "id": "e3eb0a25"
      },
      "outputs": [],
      "source": [
        "token_counts_with_rank[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're going to do some plotting. We'll use [plotly](https://plotly.com/python/) as it makes some nice interactive plots. To import it and make it place nice in Colab, we use the following code."
      ],
      "metadata": {
        "id": "nn53GGA3CZDc"
      },
      "id": "nn53GGA3CZDc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7c4655a",
      "metadata": {
        "id": "f7c4655a"
      },
      "outputs": [],
      "source": [
        "import plotly.io as pio\n",
        "pio.renderers.default='colab'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the token ranks against their frequency. You can move your mouse of points to see which token it is."
      ],
      "metadata": {
        "id": "jF2h8M-iCiK6"
      },
      "id": "jF2h8M-iCiK6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62edfcd0",
      "metadata": {
        "id": "62edfcd0"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "fig = px.scatter(x=[ i for i,t,c in token_counts_with_rank ],\n",
        "                 y=[ c for i,t,c in token_counts_with_rank ],\n",
        "                 hover_name=[ t for i,t,c in token_counts_with_rank ],\n",
        "                 labels={\n",
        "                     \"x\": \"Rank\",\n",
        "                     \"y\": \"Frequency\"\n",
        "                 })\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that there are a small number of tokens that have fairly high frequency while most have very small frequency. If we log-transform both the frequency and the rank, we get a smoother curve. This means that a small number of tokens no longer dominate."
      ],
      "metadata": {
        "id": "KC13GUMlCuia"
      },
      "id": "KC13GUMlCuia"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "714d921a",
      "metadata": {
        "id": "714d921a"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(x=[ i for i,t,c in token_counts_with_rank ],\n",
        "                 y=[ c for i,t,c in token_counts_with_rank ],\n",
        "                 hover_name=[ t for i,t,c in token_counts_with_rank ],\n",
        "                 labels={\n",
        "                     \"x\": \"Log Rank\",\n",
        "                     \"y\": \"Log Frequency\"\n",
        "                 },\n",
        "                log_x=True,\n",
        "                log_y=True)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating similarity with sparse vectors\n",
        "\n",
        "Back to our sparse vectors! We've calculated TF-IDF vectors but now we want to be able to compare them.\n",
        "\n",
        "**Exercise:** Write a function that calculates the Euclidean distance between two sparse vectors.  The `math.sqrt` function may be useful as well as using the `.get` method for the sparse vectors.\n",
        "\n",
        "Recall that the Euclidean between two $ N $ dimensional vectors ($ v $ and $ w $) is defined as: $ \\sqrt{\\sum_i^N{(v_i - w_i)^2}} $. Think the hypotenuse of a right angled triangle, but in more dimensions."
      ],
      "metadata": {
        "id": "Z9SsfR0LJL7Q"
      },
      "id": "Z9SsfR0LJL7Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94297bfe",
      "metadata": {
        "id": "94297bfe"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def sparse_euclidean_distance(sv1, sv2):\n",
        "  # your code!\n",
        "\n",
        "sv1 = {0:2, 2:1, 4:1}\n",
        "sv2 = {0:1, 1:1, 2:3}\n",
        "\n",
        "# Example usage:\n",
        "sparse_euclidean_distance(sv1,sv2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labtest(sparse_euclidean_distance)"
      ],
      "metadata": {
        "id": "UN3hWoe0xrQv"
      },
      "id": "UN3hWoe0xrQv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Euclidean distance is actually rather computationally expensive. A more common method is the angle between two vectors. We use the cosine rule to calculate it which uses the dot product. One way to optimise this approach is to normalize all your vectors so that they have length 1.\n",
        "\n",
        "**Exercise:** Write a function `normalize_sparse_vector` that takes a sparse vector and returns a normalized version of it. This is a vector pointing in the same direction but with unit length (=1)"
      ],
      "metadata": {
        "id": "Zhx7pbZnDyib"
      },
      "id": "Zhx7pbZnDyib"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46bb9d91",
      "metadata": {
        "id": "46bb9d91"
      },
      "outputs": [],
      "source": [
        "def normalize_sparse_vector(sv):\n",
        "  # your code!\n",
        "\n",
        "# Example usage:\n",
        "normalize_sparse_vector({1:1,2:2})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labtest(normalize_sparse_vector)"
      ],
      "metadata": {
        "id": "MIABEd3RxuCO"
      },
      "id": "MIABEd3RxuCO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that two vectors pointing in the same direction will have an angle of zero between them and a cosine of one. Whereas two angles that are orthogonal will have an angle of ninety degrees and a cosine of zero.\n",
        "\n",
        "To calculate the cosine of an angle between two normalized vectors, you calculate a dot product.\n",
        "\n",
        "**Exercise:** Write a function `sparse_dot_prod` that calculates the dot product between two vectors. It can assume that the vectors it is given are already normalized. Recall that the dot product between two $N$ dimensional vectors ($v$ and $w$) is defined as $ \\sum_i^N{v_i w_i} $."
      ],
      "metadata": {
        "id": "iIS5-kqUESan"
      },
      "id": "iIS5-kqUESan"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2533f6a8",
      "metadata": {
        "id": "2533f6a8"
      },
      "outputs": [],
      "source": [
        "def sparse_dot_prod(sv1, sv2):\n",
        "  # your code!\n",
        "\n",
        "sv1 = normalize_sparse_vector({0:2, 2:1, 4:1})\n",
        "sv2 = normalize_sparse_vector({0:1, 1:1, 2:3})\n",
        "\n",
        "# Example usage:\n",
        "sparse_dot_prod(sv1,sv2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labtest(sparse_dot_prod)"
      ],
      "metadata": {
        "id": "ygf0n-GrxxAu"
      },
      "id": "ygf0n-GrxxAu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, let's combine these two ideas together to create a single function that calculates the cosine similarity. Recall that the equation for the cosine similarity is:"
      ],
      "metadata": {
        "id": "gkEqEk7pfrU4"
      },
      "id": "gkEqEk7pfrU4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "FoCWgRtIfr4j"
      },
      "id": "FoCWgRtIfr4j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Write a `sparse_cosine_similarity` function that takes two sparse vectors and calculates the cosine similarity. The input vectors may not be normalized. One way would be to use `normalize_sparse_vector` then `sparse_dot_prod`, but that could be inefficient for vectors with a lot of non-zero elements as it would be using more memory to store the normalized versions. Try to calculate the cosine similarity without using `normalize_sparse_vector` by calculating the magnitudes of the two vectors and use the equation above."
      ],
      "metadata": {
        "id": "kcUAgx0pftAe"
      },
      "id": "kcUAgx0pftAe"
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_cosine_similarity(sv1, sv2):\n",
        "  # your code!\n",
        "\n",
        "# Example usage:\n",
        "sparse_cosine_similarity({0:1,1:2},{0:2,1:3})"
      ],
      "metadata": {
        "id": "3Gjwxaaqf7KP"
      },
      "id": "3Gjwxaaqf7KP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labtest(sparse_cosine_similarity)"
      ],
      "metadata": {
        "id": "l-j1cn3of_Jo"
      },
      "id": "l-j1cn3of_Jo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance on our Reddit data\n",
        "\n",
        "First, let's calculate the TFIDF vectors for our Reddit posts using our previous `make_tfidf_sparse` function,"
      ],
      "metadata": {
        "id": "8yR52NKNJE-g"
      },
      "id": "8yR52NKNJE-g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc29ec1b",
      "metadata": {
        "id": "cc29ec1b"
      },
      "outputs": [],
      "source": [
        "posts_tokens = [ p['tokens'] for p in posts ]\n",
        "posts_vocab = make_vocabulary(posts_tokens)\n",
        "posts_docfreq = doc_frequency(posts_tokens)\n",
        "N = len(posts)\n",
        "\n",
        "posts_tfidf_vectors = [ make_tfidf_sparse(pt, posts_vocab, posts_docfreq, N) for pt in posts_tokens ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use our dot product function, we first need to create a normalized version of the vectors."
      ],
      "metadata": {
        "id": "_OctQ2WqE3y9"
      },
      "id": "_OctQ2WqE3y9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2bccb63",
      "metadata": {
        "id": "b2bccb63"
      },
      "outputs": [],
      "source": [
        "posts_norm_vectors = [ normalize_sparse_vector(sv) for sv in posts_tfidf_vectors ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's do a little performance calculation. We can use the `%timeit` macro to get our notebook to calculate the time for getting the euclidean distance between one vector and the rest of them."
      ],
      "metadata": {
        "id": "SLKEjl41FWEb"
      },
      "id": "SLKEjl41FWEb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "893b0f26",
      "metadata": {
        "id": "893b0f26"
      },
      "outputs": [],
      "source": [
        "%timeit [ sparse_euclidean_distance(posts_tfidf_vectors[0],sv) for sv in posts_tfidf_vectors ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does that compare to calculating the cosine similarity?"
      ],
      "metadata": {
        "id": "WW8j0_PMFk-Q"
      },
      "id": "WW8j0_PMFk-Q"
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit [ sparse_cosine_similarity(posts_tfidf_vectors[0],sv) for sv in posts_tfidf_vectors ]"
      ],
      "metadata": {
        "id": "jooUHxgemB3A"
      },
      "id": "jooUHxgemB3A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And lastly, how does it compare to calculating the cosine similarity by doing the dot products of normalized vectors?"
      ],
      "metadata": {
        "id": "SywebNCVmGBL"
      },
      "id": "SywebNCVmGBL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be6b67d7",
      "metadata": {
        "id": "be6b67d7"
      },
      "outputs": [],
      "source": [
        "%timeit [ sparse_dot_prod(posts_norm_vectors[0],sv) for sv in posts_norm_vectors ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dot product function should be the fastest of the three. Admittedly, some preprocessing (the normalization) was needed, but if you are calculating a lot of similarities, it's clearly the best option. And that's without all the performance enhancements that modern CPUs have specifically for doing vector calculations which are perfect for dot products."
      ],
      "metadata": {
        "id": "rtKIg114FuVY"
      },
      "id": "rtKIg114FuVY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating similarity on our Reddit data\n",
        "\n",
        "We'll go back to our Reddit posts. Let's remind ourselves how many posts there are."
      ],
      "metadata": {
        "id": "k2hLvRhCGD89"
      },
      "id": "k2hLvRhCGD89"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd7ae79f",
      "metadata": {
        "id": "dd7ae79f"
      },
      "outputs": [],
      "source": [
        "len(posts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we have a matching number of normalized sparse vectors:"
      ],
      "metadata": {
        "id": "RC_w7k7pG3lH"
      },
      "id": "RC_w7k7pG3lH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3b15c6",
      "metadata": {
        "id": "cc3b15c6"
      },
      "outputs": [],
      "source": [
        "len(posts_norm_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the same code from last lab to iterate through our posts, calculate a similarity score and save it. Recall that the dot product of normalized vectors is giving us the cosine between the TFIDF vectors for the Reddit posts."
      ],
      "metadata": {
        "id": "ZbTYmj7fG-cM"
      },
      "id": "ZbTYmj7fG-cM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68b993c1",
      "metadata": {
        "id": "68b993c1"
      },
      "outputs": [],
      "source": [
        "scores_with_posts = []\n",
        "target_vector = posts_norm_vectors[0]\n",
        "for sv,post in zip(posts_norm_vectors,posts):\n",
        "  similarity_score = sparse_dot_prod(target_vector,sv)\n",
        "  scores_with_posts.append( (similarity_score, post))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we sort them. We again use `reverse=True` to get the highest scores first, and use the `key` parameter to only sort by the similarity score (and not try to sort the posts themselves which will fail)."
      ],
      "metadata": {
        "id": "mlAoLwVUHVWU"
      },
      "id": "mlAoLwVUHVWU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a830e44",
      "metadata": {
        "id": "6a830e44"
      },
      "outputs": [],
      "source": [
        "sorted_scores_with_posts = sorted(scores_with_posts, reverse=True, key=lambda x:x[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And output the top ten:"
      ],
      "metadata": {
        "id": "YlaJYX3KHhVY"
      },
      "id": "YlaJYX3KHhVY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f970523",
      "metadata": {
        "id": "2f970523"
      },
      "outputs": [],
      "source": [
        "for score,post in sorted_scores_with_posts[:10]:\n",
        "    print(f\"{score:.2f}\\t{post['title']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fantastic! We see various similar posts. There seem to be more about Irn Bru this time and about sodas"
      ],
      "metadata": {
        "id": "ufR7r0leHlW1"
      },
      "id": "ufR7r0leHlW1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using scikit-learn for vectorizing text\n",
        "\n",
        "Well done! You wrote a vectorizer from scratch. We're now going to look at scikit-learn's vectorizer functionality.\n",
        "\n",
        "Scikit-learn provides some excellent documentation. On most pages, there's a short bit of example code. This exercise is going to illustrate using scikit-learn's documentation.\n",
        "\n",
        "**Exercise**: Using the documentation of the [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class, write a function `tfidf_vectorize_with_sklearn` that uses a vectorizer to fit and transform a text corpus and returns the result which is a sparse matrix. Look at the example code on the documentation page, about halfway down the page. Yes, this is basically a copy and paste exercise . Remember to `import` the TfidfVectorizer and use the defaults for TfidfVectorizer - i.e. don't change anything."
      ],
      "metadata": {
        "id": "nQ-7kOkDI_BI"
      },
      "id": "nQ-7kOkDI_BI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ffe929",
      "metadata": {
        "id": "25ffe929"
      },
      "outputs": [],
      "source": [
        "def tfidf_vectorize_with_sklearn(text_corpus):\n",
        "  # your code!\n",
        "\n",
        "# Example usage:\n",
        "tfidf_vectorize_with_sklearn(\n",
        "  [\n",
        "    \"My favourite soft drink is Apple Tango, but I also love Irn Bru.\",\n",
        "    \"Irn Bru is a great drink.\",\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b99cf7b0",
      "metadata": {
        "id": "b99cf7b0"
      },
      "outputs": [],
      "source": [
        "labtest(tfidf_vectorize_with_sklearn)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at what the TfidfVectorizer is giving us:"
      ],
      "metadata": {
        "id": "cxfJEvWJIpB9"
      },
      "id": "cxfJEvWJIpB9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e56f2756",
      "metadata": {
        "id": "e56f2756"
      },
      "outputs": [],
      "source": [
        "sparse_matrix = tfidf_vectorize_with_sklearn(\n",
        "  [\n",
        "    \"My favourite soft drink is Apple Tango, but I also love Irn Bru.\",\n",
        "    \"Irn Bru is a great drink.\",\n",
        "  ]\n",
        ")\n",
        "\n",
        "type(sparse_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's a `csr_matrix` from the [scipy](https://scipy.org/) library which provides various data structures. This is one of them. It's a sparse matrix, similar in concept to our sparse vectors. It only store the locations and data of non-zero values.\n",
        "\n",
        "Below is some code showing how you can create one of these:"
      ],
      "metadata": {
        "id": "6lMNFI3yIvii"
      },
      "id": "6lMNFI3yIvii"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8423419",
      "metadata": {
        "id": "e8423419"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "vector = [0,2,0,3,0,1]\n",
        "sparse_matrix = csr_matrix(vector)\n",
        "sparse_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the description says \"3 stored elements\". It hasn't stored the zero elements.\n",
        "\n",
        "Another useful thing is to know the size of a matrix:"
      ],
      "metadata": {
        "id": "J2kUQ1ktJJxj"
      },
      "id": "J2kUQ1ktJJxj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7270e09c",
      "metadata": {
        "id": "7270e09c"
      },
      "outputs": [],
      "source": [
        "sparse_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can get a dense version of your matrix using the `todense` function:"
      ],
      "metadata": {
        "id": "Ceu4fXUSJUWs"
      },
      "id": "Ceu4fXUSJUWs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "627f6178",
      "metadata": {
        "id": "627f6178"
      },
      "outputs": [],
      "source": [
        "sparse_matrix.todense()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** With a very small change to the previous function, write a function `tfidf_vectorize_with_sklearn_and_spacy` that combines the `TfidfVectorizer` with our previously written `text_pipeline_spacy`. The tokenizer argument (as described on the [documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)) may come in hand. You can set the tokenizer argument `token_pattern=None` if a warning comes up about it being unused."
      ],
      "metadata": {
        "id": "_fMzbV0KJb-K"
      },
      "id": "_fMzbV0KJb-K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa6846a5",
      "metadata": {
        "id": "fa6846a5"
      },
      "outputs": [],
      "source": [
        "def tfidf_vectorize_with_sklearn_and_spacy(text_corpus):\n",
        "  # your code!\n",
        "\n",
        "# Example usage:\n",
        "tfidf_vectorize_with_sklearn_and_spacy(\n",
        "    [\n",
        "        \"My favourite soft drink is Apple Tango, but I also love Irn Bru.\",\n",
        "        \"Irn Bru is a great drink.\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a690174",
      "metadata": {
        "id": "1a690174"
      },
      "outputs": [],
      "source": [
        "labtest(tfidf_vectorize_with_sklearn_and_spacy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the moment, we're creating a vectorizer and then throwing it away. You may want to keep it if you want to vectorize future documents, or look up which element of the vector corresponds to which token (e.g. what our `make_vocabulary` function gives us). You can get the list of tokens used for mapping from the vectorizer's `get_feature_names_out` function."
      ],
      "metadata": {
        "id": "s4Zt_nt3KJV8"
      },
      "id": "s4Zt_nt3KJV8"
    },
    {
      "cell_type": "code",
      "source": [
        "text_corpus = ['Glasgow smiles better', 'He smiles a lot']\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=text_pipeline_spacy, token_pattern=None)\n",
        "X = vectorizer.fit_transform(text_corpus)\n",
        "vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "4B-RQqSuHR1H"
      },
      "id": "4B-RQqSuHR1H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run the TfidfVectorizer on the Reddit data. It may take a minute (and there's no way to get a nice progress bar )"
      ],
      "metadata": {
        "id": "NHAx4Gq3Kd6_"
      },
      "id": "NHAx4Gq3Kd6_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac0f3f5f",
      "metadata": {
        "id": "ac0f3f5f"
      },
      "outputs": [],
      "source": [
        "texts_of_posts = [ p['title'] + '\\n' + p['body'] for p in posts ]\n",
        "post_tfidf_matrix = tfidf_vectorize_with_sklearn_and_spacy(texts_of_posts)\n",
        "post_tfidf_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've now got our sparse matrix for the 2000 posts. We can double check that the sizes match:"
      ],
      "metadata": {
        "id": "i_AUr5TVKumu"
      },
      "id": "i_AUr5TVKumu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f460106c",
      "metadata": {
        "id": "f460106c"
      },
      "outputs": [],
      "source": [
        "print(f\"len(posts)={len(posts)}\")\n",
        "print(f\"post_tfidf_matrix.shape={post_tfidf_matrix.shape}\")\n",
        "\n",
        "assert len(posts) == post_tfidf_matrix.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`TfidfVectorizer` very nicely provides normalized vectors as output by default. This means we can calculate the dot product using these vectors to get the cosine similarities. Scipy provides a `dot` function that does a dot product with another vector.\n",
        "\n",
        "To dot product a (1,9828) vector with another vector of the same size, you need to transpose the second vector. For this, you can use `.T` as below.\n",
        "\n",
        "The code below calculates the dot product for our target vector against all the others."
      ],
      "metadata": {
        "id": "Si9HP-T9K3PP"
      },
      "id": "Si9HP-T9K3PP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72e56981",
      "metadata": {
        "id": "72e56981"
      },
      "outputs": [],
      "source": [
        "scores_with_posts = []\n",
        "target_vector = post_tfidf_matrix[0,:]\n",
        "for i,post in enumerate(posts):\n",
        "  dotprod = target_vector.dot(post_tfidf_matrix[i,:].T)\n",
        "  similarity_score = dotprod[0,0]\n",
        "  scores_with_posts.append( (similarity_score, post))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We sort and output the top ten. Looks pretty similar, but oddly not exactly the same as our code. This is because scikit-learn uses slightly different definitions of TF and IDF for the TF-IDF calculation. Some of the parameters to TfidfVectorizer can adjust which equations to use."
      ],
      "metadata": {
        "id": "vDfK8dl8a_DD"
      },
      "id": "vDfK8dl8a_DD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af3debd1",
      "metadata": {
        "id": "af3debd1"
      },
      "outputs": [],
      "source": [
        "sorted_scores_with_posts = sorted(scores_with_posts, reverse=True, key=lambda x:x[0])\n",
        "\n",
        "for score,post in sorted_scores_with_posts[:10]:\n",
        "    print(f\"{score:.2f}\\t{post['title']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One big benefit of using scikit-learn and scipy's sparse matrices (apart from shorter code) is performance. We can calculate an all-by-all comparison by matrix multiplying the sparse matrix of normalized TFIDF vectors by itself transposed (which uses the `dot` function again)."
      ],
      "metadata": {
        "id": "hc3BsiIkbNag"
      },
      "id": "hc3BsiIkbNag"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f7201b",
      "metadata": {
        "id": "53f7201b"
      },
      "outputs": [],
      "source": [
        "all_pairs_comparison = post_tfidf_matrix.dot(post_tfidf_matrix.T)\n",
        "all_pairs_comparison = all_pairs_comparison.todense() # It's dense data now, so let's store it as dense\n",
        "\n",
        "all_pairs_comparison.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us a 2000x2000 matrix where element `(i,j)` is the cosine similarity between post `i` and post `j`. You could then slice out row `i` to see the similarity scores of all documents against post `i`."
      ],
      "metadata": {
        "id": "cpfHAZKZb984"
      },
      "id": "cpfHAZKZb984"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End\n",
        "\n",
        "This is the end of Lab 2. Let us know if you encounter any issues! Remember we are here to help!\n",
        "\n",
        "In this lab, you...\n",
        "- built a text vectorizer from scratch\n",
        "- learned about dense and sparse data representations\n",
        "- implemented tf-idf methods to deal with term and document frequencies\n",
        "- used scikit-learn to vectorize documents\n",
        "\n",
        "**Please submit your lab through Moodle. We don't mark the labs but it helps us to craft better labs in the future**"
      ],
      "metadata": {
        "id": "5ssdhUxRX8kz"
      },
      "id": "5ssdhUxRX8kz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Extras\n",
        "\n",
        "These are optional directions to go into more detail.\n",
        "\n",
        "1. Scikit-learn provides a HashingVectorizer as an alternative to a TfidfVectorizer. Can you find out why?\n",
        "2. Vector similarity scores can be used for all sorts of applications, including clustering. Scikit-learn provides many built-in clustering implementations. This includes [hierachical clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) we discussed in lecture. Can you find any interesting clusters using this technique?\n"
      ],
      "metadata": {
        "id": "riuA2RstLxJy"
      },
      "id": "riuA2RstLxJy"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}